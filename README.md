# AI-Powered ML Notebook Generator

An intelligent web application that automatically generates complete, production-ready Jupyter notebooks with AI-generated theory sections and advanced hyperparameter tuning.

## Features

### Core Capabilities

- **Automated Notebook Generation**: Upload a CSV dataset and get a complete ML pipeline notebook
- **AI-Generated Theory**: Each model includes comprehensive theory sections generated by Gemini AI covering:
  - Mathematical foundations
  - Algorithm explanations
  - Key parameters and their effects
  - Use cases and best practices
  - Advantages and limitations
- **Multiple Output Formats**: Download as `.ipynb` (Jupyter Notebook) or `.py` (Python script)
- **Intelligent Dataset Analysis**: AI automatically detects:
  - Problem type (classification/regression/clustering)
  - Suggested target columns
  - Recommended ML models

### Generated Notebook Structure (18 Cells)

1. **Project Overview** - Introduction and objectives
2. **Environment Setup** - Package installation with version pinning
3. **Data Loading & Validation** - Multi-format support with quality checks
4. **Data Quality Assessment** - Missing values, duplicates, statistical summaries
5. **EDA Theory** - AI-generated educational content
6. **Exploratory Data Analysis** - Visualizations and correlations
7. **Preprocessing Theory** - AI-generated educational content
8. **Data Preprocessing Pipeline** - Complete preprocessing with best practices
9. **Feature & Target Separation** - Intelligent column handling
10. **Train-Test-Validation Split** - 60/20/20 split with stratification
11. **Hyperparameter Tuning Theory** - AI-generated educational content
12. **Hyperparameter Tuning Cheat Sheet** - Comprehensive reference tables
13-N. **Model-Specific Sections** - For each selected model:
    - AI-generated theory section
    - Training with GridSearchCV
    - Hyperparameter tuning
    - Evaluation with visualizations
N+1. **Model Comparison** - Comprehensive comparison tables and charts
N+2. **Model Deployment** - Model serialization and deployment preparation
N+3. **Summary** - Completion checklist

### Included ML Models

- Random Forest (Classifier/Regressor)
- XGBoost
- Logistic Regression
- Linear Regression
- Support Vector Machines (SVM)
- K-Nearest Neighbors (KNN)
- Decision Tree
- Gradient Boosting
- LightGBM

### Data Preprocessing Features

- **Missing Value Handling**: Automatic imputation (median for numeric, mode for categorical)
- **Duplicate Removal**: Automatic detection and removal
- **Categorical Encoding**: Label encoding for categorical features
- **Outlier Detection**: IQR-based outlier clipping
- **Feature Scaling**: StandardScaler normalization
- **Stratified Splitting**: Maintains class distribution in splits

### Hyperparameter Tuning

- **GridSearchCV**: Exhaustive search over parameter grids
- **Cross-Validation**: 5-fold CV for robust evaluation
- **Optimized Parameter Grids**: Pre-configured ranges for each model
- **Performance Metrics**: Comprehensive scoring for classification and regression

### Visualizations Included

- Target distribution plots
- Correlation heatmaps
- Feature relationships (scatter plots)
- Missing value analysis
- Confusion matrices (classification)
- Actual vs. Predicted plots (regression)
- Model performance comparison charts
- Training progress curves

### Database Integration

The application uses Supabase to track all generated notebooks:

- **generated_notebooks**: Stores notebook metadata and content
- **generation_logs**: Tracks generation steps and status
- Row Level Security (RLS) enabled
- Public access for demo purposes (can be restricted with auth)

## Technology Stack

- **Frontend**: React 18 with TypeScript
- **Styling**: Tailwind CSS
- **Icons**: Lucide React
- **Database**: Supabase (PostgreSQL)
- **AI Integration**: Google Gemini API
- **CSV Parsing**: PapaParse
- **Build Tool**: Vite

## How It Works

### 1. Upload Dataset
- User uploads a CSV file
- Application parses and validates the data
- Extracts dataset metadata (shape, columns, dtypes)

### 2. AI Analysis
- Sends dataset sample to Gemini AI
- Receives analysis including:
  - Problem type detection
  - Suggested target columns
  - Recommended ML models

### 3. User Configuration
- Review and confirm problem type
- Select target column from dropdown
- Choose 2-5 ML models to train

### 4. Notebook Generation
- Creates notebook structure with 18+ cells
- Generates AI-powered theory sections for each model
- Includes comprehensive cheat sheets
- Adds complete code for:
  - Data preprocessing
  - EDA with visualizations
  - Model training with hyperparameter tuning
  - Evaluation and comparison
  - Deployment preparation

### 5. Download
- Download as `.ipynb` for Jupyter
- Download as `.py` for Python scripts
- Generated notebooks are production-ready

## API Keys and Configuration

The application uses the following API key (already configured):
- **Gemini API Key**: `AIzaSyAcK2RJcvfDsEm0na4G65O2ucP4Ial2c-0`

Supabase configuration is loaded from environment variables:
- `VITE_SUPABASE_URL`
- `VITE_SUPABASE_ANON_KEY`

## Usage Example

1. **Start the application**:
   ```bash
   npm run dev
   ```

2. **Upload a CSV file** containing your dataset

3. **Review the AI analysis** and configuration

4. **Select models** you want to train (e.g., Random Forest, XGBoost, Logistic Regression)

5. **Click "Generate Notebook"** and wait 2-3 minutes

6. **Download** your notebook in .ipynb or .py format

7. **Open in Jupyter** and execute cells sequentially

## Example Generated Code

### Data Preprocessing
```python
df_processed = df.copy()

# Handling missing values
for col in numeric_columns:
    if df_processed[col].isnull().sum() > 0:
        df_processed[col].fillna(df_processed[col].median(), inplace=True)

# Encoding categorical variables
for col in categorical_columns:
    if col != target_col and df_processed[col].nunique() < 10:
        le = LabelEncoder()
        df_processed[col] = le.fit_transform(df_processed[col].astype(str))

# Handling outliers (IQR method)
for col in numeric_columns:
    if col != target_col:
        Q1 = df_processed[col].quantile(0.25)
        Q3 = df_processed[col].quantile(0.75)
        IQR = Q3 - Q1
        df_processed[col] = df_processed[col].clip(Q1 - 1.5*IQR, Q3 + 1.5*IQR)
```

### Hyperparameter Tuning
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)
best_model = grid_search.best_estimator_
```

## Database Schema

### generated_notebooks
- `id`: UUID primary key
- `user_id`: Text (default: 'anonymous')
- `dataset_name`: Text
- `dataset_info`: JSONB (metadata)
- `problem_type`: Text (classification/regression/clustering)
- `target_column`: Text
- `selected_models`: Text array
- `notebook_content`: Text (JSON string)
- `theory_content`: JSONB
- `status`: Text (pending/generating/completed/failed)
- `created_at`: Timestamp with timezone
- `updated_at`: Timestamp with timezone

### generation_logs
- `id`: UUID primary key
- `notebook_id`: UUID (foreign key)
- `step`: Text
- `status`: Text
- `message`: Text
- `created_at`: Timestamp with timezone

## Features in Detail

### AI-Generated Theory Sections

Each model includes detailed theory covering:
- **Mathematical Foundation**: Core equations and principles
- **Algorithm Explanation**: Step-by-step how it works
- **Key Parameters**: Hyperparameters and their effects
- **Use Cases**: When to use this model
- **Advantages**: Strengths of the approach
- **Limitations**: Weaknesses and considerations
- **Best Practices**: Tips for optimal performance

### Hyperparameter Cheat Sheets

Generated tables include:
- Model selection matrix (problem type → models → parameters)
- Tuning strategy guide (data size → method)
- Evaluation metrics reference
- Preprocessing techniques
- Troubleshooting guide

### Code Quality

- PEP-8 compliant Python code
- Comprehensive error handling
- Robust input validation
- Clear comments and documentation
- Production-ready structure

## Limitations

- Currently supports CSV files only
- Requires internet connection for AI generation
- Generation time: 2-5 minutes depending on selected models
- Maximum dataset size: Limited by browser memory

## Future Enhancements

- Support for additional file formats (Excel, JSON, Parquet)
- GitHub/Kaggle repository integration
- More hyperparameter tuning strategies (Bayesian, Hyperband)
- Deep learning models (CNN, RNN, LSTM)
- Advanced visualization options
- User authentication and saved notebooks
- Collaborative features
- Model deployment to cloud platforms

## License

MIT License

## Credits

- Built with React, TypeScript, and Tailwind CSS
- AI-powered by Google Gemini API
- Database powered by Supabase
- UI icons by Lucide React

---

**Powered by AI** • **Built for Data Scientists** • **Production-Ready Code**
